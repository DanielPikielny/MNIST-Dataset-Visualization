# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wq1ieSZnGdwFHk3HRkS3MLtamFNjM8Ou
"""

import torchvision
from torchvision import datasets

# Define a convenient location for the dataset
dataset_path = "./mnist_data"

# Download the MNIST dataset
mnist_data = datasets.MNIST(root=dataset_path, download=True)

import matplotlib.pyplot as plt
import torch

# Load the dataset (Ensure it's downloaded first)
dataset_path = "./mnist_data"
mnist_data = datasets.MNIST(root=dataset_path, train=True, download=True)

# Extract samples for each class
samples_per_class = 5
classes = list(range(10))
class_images = {digit: [] for digit in classes}

# Go through the dataset and collect samples
for img, label in mnist_data:
    if len(class_images[label]) < samples_per_class:
        class_images[label].append(img)
    if all(len(class_images[digit]) >= samples_per_class for digit in classes):
        break  # Stop once we have enough samples for each class

# Plot images for each class
fig, axes = plt.subplots(nrows=10, ncols=samples_per_class, figsize=(10, 20))
for digit in classes:
    for i, img in enumerate(class_images[digit]):
        axes[digit, i].imshow(img, cmap="gray")  # Apply grayscale colormap
        axes[digit, i].axis("off")
        axes[digit, i].set_title(f"{digit}", fontsize=12)  # Label the class

plt.tight_layout()
plt.show()

import torch
import torchvision.transforms as transforms
import random
import numpy as np
from torchvision import datasets
from PIL import Image

# Load MNIST dataset
dataset_path = "./mnist_data"
mnist_data = datasets.MNIST(root=dataset_path, train=True, download=True)

# Define transformation
transform = transforms.ToTensor()

# Function to create three-digit numbers
def create_three_digit_samples(mnist_data, num_samples_per_class=4000):
    digit_dict = {str(i): [] for i in range(10)}

    # Organize images by digit
    for img, label in mnist_data:
        digit_dict[str(label)].append(img)

    # Create dataset for numbers 000 to 100
    mnist101_data = []
    for num in range(101):  # '000' to '100'
        num_str = f"{num:03d}"  # Convert to three-digit format
        digit_imgs = [random.choice(digit_dict[d]) for d in num_str]

        # Combine digits into a single image (horizontally)
        combined_img = Image.new("L", (digit_imgs[0].width * 3, digit_imgs[0].height))
        for i, digit_img in enumerate(digit_imgs):
            combined_img.paste(digit_img, (i * digit_imgs[0].width, 0))

        # Add to dataset multiple times to meet sample count
        mnist101_data.extend([transform(combined_img)] * num_samples_per_class)

    return mnist101_data

# Create the MNIST101 dataset
mnist101_samples = create_three_digit_samples(mnist_data)

# Confirm dataset size
print(f"MNIST101 dataset created with {len(mnist101_samples)} samples!")

import matplotlib.pyplot as plt

# Select a few random samples to visualize
num_samples_to_display = 10
selected_samples = random.sample(mnist101_samples, num_samples_to_display)

# Plot samples
fig, axes = plt.subplots(1, num_samples_to_display, figsize=(15, 5))
for i in range(num_samples_to_display):
    axes[i].imshow(selected_samples[i].squeeze(), cmap="gray")
    axes[i].axis("off")

plt.show()

import albumentations as A
import cv2
import numpy as np
import torch
from torchvision import transforms
from PIL import Image

# Define augmentations
augmentations = A.Compose([
    A.Rotate(limit=20, p=0.5),  # Rotate up to Â±20 degrees
    A.GaussianBlur(blur_limit=(3, 7), p=0.3),  # Gaussian Blur
    A.MedianBlur(blur_limit=5, p=0.3),  # Median Blur
    A.GaussNoise(p=0.4),  # Gaussian Noise (Fixed)
    A.OpticalDistortion(distort_limit=0.3, p=0.3),  # Elastic Distortion
    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),  # Brightness & Contrast
    A.MotionBlur(blur_limit=7, p=0.3),  # Motion Blur
    A.Perspective(scale=(0.05, 0.1), p=0.3),  # Perspective Transform
    A.CLAHE(clip_limit=4, p=0.3),  # Sharpening using CLAHE
    A.CoarseDropout(p=0.3),  # Fixed: No unnecessary arguments
    A.RandomScale(scale_limit=0.2, p=0.3),  # Random Zoom
    A.ToGray(p=0.3),  # Convert to grayscale
    A.InvertImg(p=0.2),  # Invert colors
])

# Function to apply augmentation
def apply_augmentation(image):
    img_array = np.array(image)  # Convert to NumPy array
    augmented = augmentations(image=img_array)['image']  # Apply augmentation
    return Image.fromarray(augmented)  # Convert back to PIL Image

# Test Augmentations
test_img = mnist_data[0][0]  # Get first sample
augmented_img = apply_augmentation(test_img)

# Display Original & Augmented
import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].imshow(test_img, cmap="gray")
ax[0].set_title("Original")
ax[0].axis("off")

ax[1].imshow(augmented_img, cmap="gray")
ax[1].set_title("Augmented")
ax[1].axis("off")

plt.show()

# Display a batch of augmented samples
num_samples = 15
fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))

# Select random images and apply augmentations
random_indices = np.random.randint(0, len(mnist101_samples), num_samples)
original_samples = [mnist101_samples[i] for i in random_indices]

# Convert Tensor images to PIL format before augmentation
augmented_samples = [apply_augmentation(transforms.ToPILImage()(img)) for img in original_samples]

# Plot images before and after augmentation
for i in range(num_samples):
    # Original Image
    axes[0, i].imshow(original_samples[i].squeeze(0), cmap="gray")  # Fixed: Remove extra dimension
    axes[0, i].axis("off")
    axes[0, i].set_title("Original")

    # Augmented Image
    axes[1, i].imshow(augmented_samples[i], cmap="gray")  # Fixed: Augmented images are already PIL
    axes[1, i].axis("off")
    axes[1, i].set_title("Augmented")

plt.tight_layout()
plt.show()

import torch
from torch.utils.data import Dataset
import random
from torchvision import transforms
from PIL import Image

class MNIST101Dataset(Dataset):
    def __init__(self, mnist_data, num_samples_per_class=4000, transform=None):
        """
        Custom dataset class for MNIST101
        :param mnist_data: Original MNIST dataset
        :param num_samples_per_class: Number of samples per class (default 4000)
        :param transform: Optional image transformations
        """
        self.transform = transform
        self.data = []
        self.labels = []

        # Organize MNIST images by digit
        digit_dict = {str(i): [] for i in range(10)}
        for img, label in mnist_data:
            digit_dict[str(label)].append(img)

        # Generate dataset for numbers 000 to 100
        for num in range(101):  # '000' to '100'
            num_str = f"{num:03d}"  # Convert number to three-digit string
            for _ in range(num_samples_per_class):
                digit_imgs = [random.choice(digit_dict[d]) for d in num_str]

                # Combine digits into a single image horizontally
                combined_img = Image.new("L", (digit_imgs[0].width * 3, digit_imgs[0].height))
                for i, digit_img in enumerate(digit_imgs):
                    combined_img.paste(digit_img, (i * digit_imgs[0].width, 0))

                # Apply transformations if provided
                if self.transform:
                    combined_img = self.transform(combined_img)

                self.data.append(combined_img)
                self.labels.append(num)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img = self.data[idx]
        label = self.labels[idx]
        return img, label

from torch.utils.data import DataLoader

# Define transformations (resize for consistency)
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert PIL image to tensor
    transforms.Normalize((0.5,), (0.5,)),  # Normalize pixel values
])

# Create MNIST101 dataset
mnist101_dataset = MNIST101Dataset(mnist_data, num_samples_per_class=4000, transform=transform)

# Create DataLoader for batch processing
mnist101_loader = DataLoader(mnist101_dataset, batch_size=64, shuffle=True)

# Test: Retrieve a batch of images
batch_images, batch_labels = next(iter(mnist101_loader))
print(f"Batch size: {batch_images.shape}, Labels: {batch_labels[:10]}")

import torch
from torchvision import datasets, transforms
from torch.utils.data import Dataset

# Define transforms to convert images to tensors
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load MNIST train and test subsets
train_mnist10 = datasets.MNIST(root="./mnist_data", train=True, download=True, transform=transform)
test_mnist10 = datasets.MNIST(root="./mnist_data", train=False, download=True, transform=transform)

import random
from PIL import Image
from torchvision import transforms

def create_mnist101_subset(mnist_data, num_samples_per_class=4000):
    """
    Constructs the MNIST101 dataset subset using MNIST data
    :param mnist_data: MNIST10 subset (train or test)
    :param num_samples_per_class: Number of samples per class (default 4000)
    :return: List of three-digit number images & their labels
    """
    digit_dict = {str(i): [] for i in range(10)}

    # Organize digits in MNIST10 subset
    for img, label in mnist_data:
        digit_dict[str(label)].append(img)

    mnist101_data, mnist101_labels = [], []

    for num in range(101):  # Numbers from "000" to "100"
        num_str = f"{num:03d}"  # Convert to three-digit format
        for _ in range(num_samples_per_class):  # Repeat 4000 times
            # Convert Tensors to PIL Images before processing
            digit_imgs = [transforms.ToPILImage()(random.choice(digit_dict[d])) for d in num_str]

            # Combine digits into a single horizontally stacked image
            combined_img = Image.new("L", (digit_imgs[0].width * 3, digit_imgs[0].height))
            for i, digit_img in enumerate(digit_imgs):
                combined_img.paste(digit_img, (i * digit_imgs[0].width, 0))

            mnist101_data.append(combined_img)
            mnist101_labels.append(num)

    return mnist101_data, mnist101_labels

# Generate MNIST101 Train & Test Subsets
mnist101_train_data, mnist101_train_labels = create_mnist101_subset(train_mnist10)
mnist101_test_data, mnist101_test_labels = create_mnist101_subset(test_mnist10)

from torch.utils.data import Dataset

class MNIST101Dataset(Dataset):
    """
    Custom dataset class for MNIST101
    """
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img = self.data[idx]
        label = self.labels[idx]

        # Apply transformations if provided
        if self.transform:
            img = self.transform(img)

        return img, label

# Create MNIST101 Train & Test Datasets
mnist101_train_dataset = MNIST101Dataset(mnist101_train_data, mnist101_train_labels, transform=transforms.ToTensor())
mnist101_test_dataset = MNIST101Dataset(mnist101_test_data, mnist101_test_labels, transform=transforms.ToTensor())

from torch.utils.data import DataLoader

# Load Data with DataLoader
train_loader = DataLoader(mnist101_train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(mnist101_test_dataset, batch_size=64, shuffle=False)

# Test: Retrieve a batch
train_batch_images, train_batch_labels = next(iter(train_loader))
print(f"Train Batch: {train_batch_images.shape}, Labels: {train_batch_labels[:10]}")

test_batch_images, test_batch_labels = next(iter(test_loader))
print(f"Test Batch: {test_batch_images.shape}, Labels: {test_batch_labels[:10]}")

import collections

# Count occurrences of each label
train_counts = collections.Counter(mnist101_train_labels)
test_counts = collections.Counter(mnist101_test_labels)

# Print class distributions
print("MNIST101 Train Class Counts:", dict(train_counts))
print("MNIST101 Test Class Counts:", dict(test_counts))

# Retrieve one batch from train and test sets
train_batch_images, train_batch_labels = next(iter(train_loader))
test_batch_images, test_batch_labels = next(iter(test_loader))

# Print dataset properties
print(f"Train Batch Size: {train_batch_images.shape}, Labels: {train_batch_labels[:10]}")
print(f"Test Batch Size: {test_batch_images.shape}, Labels: {test_batch_labels[:10]}")

import matplotlib.pyplot as plt

num_samples = 10
fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))

# Select random train samples
random_indices = torch.randint(0, len(mnist101_train_dataset), (num_samples,))
original_samples = [mnist101_train_dataset[i][0] for i in random_indices]
labels = [mnist101_train_dataset[i][1] for i in random_indices]

# Plot images
for i in range(num_samples):
    axes[0, i].imshow(original_samples[i].squeeze(), cmap="gray")
    axes[0, i].axis("off")
    axes[0, i].set_title(f"Train: {labels[i]}")

# Select random test samples
random_indices = torch.randint(0, len(mnist101_test_dataset), (num_samples,))
original_samples = [mnist101_test_dataset[i][0] for i in random_indices]
labels = [mnist101_test_dataset[i][1] for i in random_indices]

for i in range(num_samples):
    axes[1, i].imshow(original_samples[i].squeeze(), cmap="gray")
    axes[1, i].axis("off")
    axes[1, i].set_title(f"Test: {labels[i]}")

plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim

# Define an MLP model for MNIST101
class MNIST101MLP(nn.Module):
    def __init__(self):
        super(MNIST101MLP, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28 * 84, 512)  # Input layer
        self.fc2 = nn.Linear(512, 256)      # Hidden layer
        self.fc3 = nn.Linear(256, 101)      # Output layer (101 classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # No activation here since we'll use CrossEntropyLoss
        return x

# Initialize model
model = MNIST101MLP()

# Define loss function & optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
def train_model(model, train_loader, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}")

# Train model on MNIST101
train_model(model, train_loader)

# Evaluation function
def evaluate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    accuracy = 100 * correct / total
    print(f"Test Accuracy: {accuracy:.2f}%")

# Evaluate trained model on MNIST101 test set
evaluate_model(model, test_loader)

import torch

# Save the trained model
torch.save(model.state_dict(), "mnist101_mlp.pth")

from google.colab import files
files.download("mnist101_mlp.pth")

import pickle

# Save datasets
with open("mnist101_train.pkl", "wb") as f:
    pickle.dump((mnist101_train_data, mnist101_train_labels), f)

with open("mnist101_test.pkl", "wb") as f:
    pickle.dump((mnist101_test_data, mnist101_test_labels), f)

files.download("mnist101_train.pkl")
files.download("mnist101_test.pkl")